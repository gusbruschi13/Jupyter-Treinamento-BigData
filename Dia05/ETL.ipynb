{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown magic command 'pyspark'\n",
      "UnknownMagic: unknown magic command 'pyspark'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ler arquivo CSV referente aos pagamentos\n",
    "\n",
    "%pyspark\n",
    "rdd_clients = spark.read.format(\n",
    "   \"com.databricks.spark.csv\").option(\n",
    "   \"header\", \"true\").option(\n",
    "   \"inferSchema\", \"true\").option(\n",
    "   \"delimiter\", ',').load(\n",
    "   'clients.csv')\n",
    "\n",
    "rdd_clients.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Verificando Metadados da tabela\n",
    "rdd_clients.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Verificando 5 linhas da tabela\n",
    "rdd_clients.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler arquivo CSV referente aos pagamentos\n",
    "\n",
    "%pyspark\n",
    "rdd_payments = spark.read.format(\n",
    "   \"com.databricks.spark.csv\").option(\n",
    "   \"header\", \"true\").option(\n",
    "   \"inferSchema\", \"true\").option(\n",
    "   \"delimiter\", ',').load(\n",
    "   'payments.csv')\n",
    "\n",
    "rdd_payments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rdd_payments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rdd_payments.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler arquivo CSV referente a empréstimos contratados pelos clientes\n",
    "\n",
    "%pyspark\n",
    "rdd_loan = spark.read.format(\n",
    "   \"com.databricks.spark.csv\").option(\n",
    "   \"header\", \"true\").option(\n",
    "   \"inferSchema\", \"true\").option(\n",
    "   \"delimiter\", ',').load(\n",
    "   'loans.csv')\n",
    "rdd_loan.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rdd_loan.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "rdd_loan.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disponibilizando todas as tabelas RDDs para SQLContext\n",
    "\n",
    "%pyspark\n",
    "payments_dfm = rdd_payments.registerTempTable(\"payments_dfm\")\n",
    "clients_dfm = rdd_clients.registerTempTable(\"clients_dfm\")\n",
    "loans_dfm = rdd_loan.registerTempTable(\"loans_dfm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos investigar o cliente id 46109\n",
    "\n",
    "%pyspark\n",
    "spark.sql(\n",
    "             \"\"\"SELECT \n",
    "                    *,\n",
    "                    cast(loan_start as date) as DT_INICIO,\n",
    "                    cast(loan_end as date) as DT_FIM,\n",
    "                    datediff(cast(loan_end as date),cast(loan_start as date)) as QT_DIAS_START_END,\n",
    "                    round(datediff(cast(loan_end as date),cast(loan_start as date))/30.5) as QT_MESES_START_END,\n",
    "                    round(datediff(cast(loan_end as date),cast(loan_start as date))/365) as QT_ANOS_START_END\n",
    "                FROM\n",
    "                    loans_dfm \n",
    "                WHERE \n",
    "                    client_id = 46109 and loan_type in ('home') \n",
    "                order by cast(loan_start as date)    \n",
    "             \"\"\"\n",
    "         ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos investigar o contrato Id 10243 do cliente 46109 na base de pagamentos\n",
    "\n",
    "%pyspark\n",
    "spark.sql(\n",
    "             \"\"\"SELECT \n",
    "                    *,\n",
    "                    cast(payment_date as date ) as DT_PGTO\n",
    "                    --min(payment_date) as DT_PRIMEIRO_PAGAMENTO,\n",
    "                    --max(payment_date) as DT_ULTIMO_PAGAMENTO,\n",
    "                FROM\n",
    "                    payments_dfm \n",
    "                WHERE \n",
    "                    loan_id = 10243 \n",
    "             \"\"\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos investigar os pagamentos referentes ao contrato 10243 do cliente 46109\n",
    "\n",
    "%pyspark\n",
    "spark.sql(\n",
    "             \"\"\"SELECT\n",
    "                    loan_id,\n",
    "                    sum(case when missed = 1 then 0 else payment_amount end) as VL_PGTO_MISS,\n",
    "                    sum(case when missed = 0 then 0 else payment_amount end) as VL_PGTO_NOT_MISS,\n",
    "                    sum(payment_amount) as VL_TOT,\n",
    "                    cast(min(payment_date) as date) as DT_PRIMEIRO_PAGAMENTO,\n",
    "                    cast(max(payment_date) as date) as DT_ULTIMO_PAGAMENTO,\n",
    "                    count(*) as QT_PGTOS\n",
    "                FROM\n",
    "                    payments_dfm \n",
    "                WHERE \n",
    "                    loan_id = 10243\n",
    "                GROUP BY  \n",
    "                    loan_id\n",
    "             \"\"\"\n",
    "         ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar algumas Transformações nos dados\n",
    "\n",
    "%pyspark\n",
    "clients_00 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            *,\n",
    "                            round(log(income),4) as VL_LOG_RENDA,\n",
    "                            cast(joined as date) as DT_JOINED,\n",
    "                            substr(cast(joined as date),6,2) as VL_MES_JOINED\n",
    "                        FROM\n",
    "                            clients_dfm                     \n",
    "                     \"\"\"\n",
    "                     )\n",
    "clients_00_dfm = clients_00.registerTempTable(\"clients_00_dfm\")                     \n",
    "clients_00.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o valor médio, mínimo e máximo de empréstimos para cada cliente\n",
    "\n",
    "%pyspark\n",
    "loans_00 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            client_id,\n",
    "                            round(avg(loan_amount),4) as VL_MED_EMP_CLI,\n",
    "                            min(loan_amount) as VL_MIN_EMP_CLI,\n",
    "                            max(loan_amount) as VL_MAX_EMP_CLI\n",
    "                        FROM\n",
    "                            loans_dfm  \n",
    "                        group by client_id     \n",
    "                     \"\"\"\n",
    "                     )\n",
    "loans_00_dfm = loans_00.registerTempTable(\"loans_00_dfm\")                         \n",
    "loans_00.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos levar as informações calculadas para a tabela de clientes\n",
    "\n",
    "%pyspark\n",
    "clientes_01 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            a.*,\n",
    "                            b.VL_MED_EMP_CLI,\n",
    "                            b.VL_MIN_EMP_CLI,\n",
    "                            b.VL_MAX_EMP_CLI\n",
    "                        FROM\n",
    "                            clients_00_dfm as a  \n",
    "                        left join  loans_00_dfm as b\n",
    "                        on a.client_id = b.client_id\n",
    "                     \"\"\"\n",
    "                     )\n",
    "                     \n",
    "clientes_01.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar os pagamentos dos empréstimos efetuados\n",
    "\n",
    "%pyspark\n",
    "loans_02 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            a.*,\n",
    "                            b.payment_amount,\n",
    "                            b.payment_date,\n",
    "                            b.missed\n",
    "                        FROM\n",
    "                            loans_dfm as a  \n",
    "                        left join  payments_dfm as b\n",
    "                        on a.loan_id = b.loan_id\n",
    "                     \"\"\"\n",
    "                     )\n",
    "loans_02_dfm = loans_02.registerTempTable(\"loans_02_dfm\")                       \n",
    "loans_02.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantidade de pagamentos por tipo de empréstimo(Granularidade Contrato)\n",
    "\n",
    "%pyspark\n",
    "loans_03 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            loan_type,\n",
    "                            round(avg(payment_amount),2) as VL_MED_PAGAMENTOS,\n",
    "                            count(*) as QT_CONTRATOS_TIPO\n",
    "                        FROM\n",
    "                            loans_02_dfm \n",
    "                        group by 1  \n",
    "                     \"\"\"\n",
    "                     )\n",
    "loans_03_dfm = loans_03.registerTempTable(\"loans_03_dfm\")                       \n",
    "loans_03.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo MED, MIN e MAX de pagamentos para cada tipo de emprestimo (Granularidade CPF)\n",
    "\n",
    "%pyspark\n",
    "loans_04 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            client_id,\n",
    "                            sum(case when loan_type in ('cash') then 1 else 0 end) as QT_EMP_CASH,\n",
    "                            avg(case when loan_type in ('cash') then loan_amount else 0 end) as VL_MEDEMP_CASH,\n",
    "                            count(*) as QT_CONTRATOS_TIPO\n",
    "                        FROM\n",
    "                            loans_02_dfm \n",
    "                        group by 1  \n",
    "                     \"\"\"\n",
    "                     )\n",
    "loans_04_dfm = loans_04.registerTempTable(\"loans_04_dfm\")                       \n",
    "loans_04.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento de datas sistêmicas\n",
    "\n",
    "%pyspark\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import date, timedelta\n",
    "dataref = datetime.now()-timedelta(0, 0, 0, 0, 0, +2)\n",
    "anomesdia = dataref.strftime(\"%Y%m%d\")\n",
    "dataref\n",
    "#print('Data Hoje: ',dataref.strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar tabela com particionamento por data de referência\n",
    "\n",
    "%pyspark\n",
    "etl_final = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            *,\n",
    "                            '{}' as PK_DATREF\n",
    "                        FROM\n",
    "                            loans_04_dfm \n",
    "                     \"\"\".format(anomesdia)\n",
    "                     )\n",
    "             \n",
    "etl_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando Parquet não particionado com HDFS (Não visível ao Hive)\n",
    "\n",
    "%pyspark\n",
    "etl_final.write.parquet(\"etl_final_00.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando parquet não particionado no Hive\n",
    "\n",
    "%pyspark\n",
    "from pyspark.sql import HiveContext\n",
    "hive_context = HiveContext(sc)\n",
    "\n",
    "# Salvando a tabela como temporária no Hive\n",
    "#hive_context.registerDataFrameAsTable(etl_final, \"etl_final_00\")\n",
    "\n",
    "# Salvando a tabela no schema (database) treinamento como permanente do Hive (modo append)\n",
    "etl_final.write.mode('append').saveAsTable(\"treinamento.etl_final_00\")\n",
    "\n",
    "hive_context.sql(\"show tables in treinamento\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "spark.catalog.dropTempView(\"clients_00_dfm\") \n",
    "spark.catalog.dropTempView(\"clients_dfm\") \n",
    "spark.catalog.dropTempView(\"loans_00_dfm\") \n",
    "spark.catalog.dropTempView(\"loans_02_dfm\") \n",
    "spark.catalog.dropTempView(\"loans_03_dfm\") \n",
    "spark.catalog.dropTempView(\"loans_04_dfm\") \n",
    "spark.catalog.dropTempView(\"loans_dfm\") \n",
    "spark.catalog.dropTempView(\"payments_dfm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "hive_context.sql(\"show tables in treinamento\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "tab_hive = hive_context.sql(\"select * from treinamento.etl_final_00\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando parquet particionado utilizando Hive\n",
    "\n",
    "%pyspark\n",
    "# Salvando a tabela no schema (database) treinamento como permanente do Hive (modo append) particionado\n",
    "etl_final.write.mode('append').partitionBy('PK_DATREF').saveAsTable(\"treinamento.etl_final_01\")\n",
    "\n",
    "hive_context.sql(\"show tables in treinamento\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Ler partição Hive\n",
    "df_hive_parquet = sqlContext.read.option(\"mergeSchema\", \"true\").parquet(\"/user/hive/warehouse/treinamento.db/etl_final_01/PK_DATREF=20181126\")\n",
    "df_hive_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df_hive_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando parquet particionado no S3 (AWS)\n",
    "\n",
    "%pyspark\n",
    "nm_path_s3 = 's3://treinamento-big-data/'\n",
    "etl_final.write.partitionBy('PK_DATREF').parquet(nm_path_s3, mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
